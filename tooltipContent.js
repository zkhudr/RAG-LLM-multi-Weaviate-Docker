// static/tooltipContent.js
const tooltipContent = {
    // Model Section
    "LLM_TEMPERATURE": "Controls how predictable vs. creative the model’s output is. Range: 0 (fully deterministic) to 2 (very creative). We usually start around 0.3–0.7 for a good mix of accuracy and variety. :contentReference[oaicite:0]{index=0}",
    "MAX_TOKENS": "The hard limit on how long the model’s reply can be (in tokens). Think of it as the max word count. 512–1024 tokens is a solid default.",
    "OLLAMA_MODEL": "Name of your local Ollama model (e.g. 'llama3', 'mistral'). Make sure you’ve downloaded it with `ollama pull <model>`.",
    "EMBEDDING_MODEL": "Which text-to-vector model to use (e.g. 'nomic-embed-text' or 'all-MiniLM-L6-v2'). This choice affects retrieval accuracy and speed :contentReference[oaicite:1]{index=1}.",
    "TOP_P": "Nucleus sampling: only consider tokens within the top P cumulative probability. Lower = more focused; higher = more varied. Try ~0.9. :contentReference[oaicite:2]{index=2}",
    "FREQUENCY_PENALTY": "Gently discourages the model from repeating the same words. A value of 0.1–0.4 usually reduces repetition without harming coherence. :contentReference[oaicite:3]{index=3}",
    "SYSTEM_MESSAGE": "Your high-level instructions to the model—defines its role, tone, rules, and output format.",
    "EXTERNAL_API_PROVIDER": "The cloud LLM service to use (e.g. 'openai', 'anthropic'). Make sure the corresponding API key is set in your environment.",
    "EXTERNAL_API_MODEL_NAME": "(Optional) The exact model name from that provider (e.g. 'gpt-4o', 'deepseek-chat'). Overrides their default—check their docs for valid names.",

    // Retrieval Section
    "COLLECTION_NAME": "The exact Weaviate collection/class you want to query. It’s case-sensitive and must match your schema.",
    "K_VALUE": "How many top documents to fetch for each query. More documents = more context but higher latency. We often use 5–10.",
    "SCORE_THRESHOLD": "Throw away any retrieved chunks scoring below this (0–1). Use ~0.6–0.75 to filter out off-topic results.",
    "LAMBDA_MULTIPLIER": "For MMR: trades off relevance vs. diversity (0 = max diversity, 1 = pure relevance). Try around 0.5–0.7.",
    "SEARCH_TYPE": "Which retrieval strategy: 'similarity' (vector only), 'mmr' (adds diversity), or 'hybrid' (combines vector + BM25 if you’ve implemented it).",
    "DOMAIN_SIMILARITY_THRESHOLD": "Before full retrieval, discard queries if they’re too far (cosine) from your domain centroid. Helps keep results on-topic.",
    "SPARSE_RELEVANCE_THRESHOLD": "In hybrid mode, the minimum BM25 score (0–1) to keep a result. Around 0.1–0.2 often works.",
    "FUSED_RELEVANCE_THRESHOLD": "After mixing dense + sparse scores, drop anything under this combined threshold (0–1). Aim for ~0.3–0.4.",
    "SEMANTIC_WEIGHT": "In hybrid fusion, the weight for vector (dense) search. Often set between 0.5–0.8.",
    "SPARSE_WEIGHT": "In hybrid fusion, the weight for keyword (sparse) search. Usually 1 − semantic weight.",
    "PERFORM_DOMAIN_CHECK": "On/off flag for an initial vector check to make sure queries match your domain. Recommended=true.",
    "PERFORM_TECHNICAL_VALIDATION": "Skip keyword-based filters and accept whatever our pipeline returns.",
    "WEAVIATE_HOST": "Hostname or IP of your Weaviate instance. Make sure it’s reachable from this app.",
    "WEAVIATE_HTTP_PORT": "HTTP port for Weaviate (default 8080 or 8090).",
    "WEAVIATE_GRPC_PORT": "gRPC port for Weaviate (default 50051 or 50061).",
    "retrieve_with_history": "If true, we include recent chat history when generating embeddings—good for context but may dilute focus.",

    // --- Domain Vector Settings ---
    "DOMAIN_CENTROID": "Path to your pre-computed domain vector (.npy). Used for quick relevance checks.",
    "DOMAIN_KEYWORDS": "Your core, manually curated keywords for topic filtering.",
    "DIVERSITY_THRESHOLD": "Controls the minimum cosine distance between the old and new domain centroids required to trigger a recalculation (0–1). Higher values mean only larger drifts will update the centroid.",
    "AUTO_DOMAIN_KEYWORDS": "Keywords automatically generated by your build script. Overwrites on each rebuild.",
    "USER_ADDED_KEYWORDS": "Extra keywords you’ve added via the UI or config. Supplements the auto list.",

    // Security Section
    "SANITIZE_INPUT": "Turn on basic input cleaning to block simple injection attacks. Recommended=true.",
    "RATE_LIMIT": "Max requests per user per minute (e.g. 20–60). Prevents abuse and protects your backend.",
    "API_TIMEOUT": "How many seconds to wait for an external LLM response before giving up. 30–90s is typical.",
    "CACHE_ENABLED": "If enabled, identical queries (including history) are served from cache for faster replies.",

    // Document Section
    "CHUNK_SIZE": "Maximum tokens in each text chunk when indexing. Balances context vs. precision. 512–1024 tokens is common.",
    "CHUNK_OVERLAP": "How many tokens overlap between chunks (10–20% of chunk size) so context isn’t lost.",
    "FILE_TYPES": "Which file extensions we’ll ingest (e.g. '.pdf,.txt,.md').",
    "PARSE_TABLES": "If true, try to detect and extract tables during document parsing—useful for data-heavy docs.",
    "GENERATE_SUMMARY": "If true, auto-create an LLM summary when ingesting each doc. Speeds up later previews but adds processing time.",
    "DOCUMENT_DIRECTORY": "Local path where your files live for ingestion. Make sure the app can read this folder.",

    // Pipeline Section
    "max_history_turns": "How many recent Q&A pairs to include in prompts (keeps context but fits within token limits). 3–5 is a good rule of thumb.",

    // Keyword Extraction Section
    "SENTENCE_TRANSFORMER_MODEL": "Alias for your embedding model used during keyword extraction. Should match your main embedding setting.",
    "KEYWORDS_PER_DOCUMENT": "Max candidate keywords pulled from each doc before global filtering. Aim for 10–20.",
    "FINAL_KEYWORDS_COUNT": "Target total unique keywords across the entire corpus. 100–5000+, depending on size.",
    "MINIMUM_DOCUMENT_FREQUENCY": "Drop any keyword that appears in fewer than this many docs (absolute number or fraction, e.g. 3 or 0.01).",
    "DIVERSITY": "In final keyword MMR, balance relevance vs. diversity (0 = pure relevance, 1 = pure diversity). 0.5–0.8 is typical.",
    "DISABLE_POS_FILTERING": "Turn off part-of-speech filtering if you need to keep acronyms or codes that POS filters might drop.",
    "COLLECTION_NAME": "The name of the collection where your documents are stored.",
  "CENTROID_COLLECTION": "The collection whose vectors are used to calculate and store the centroid.",
  "K_VALUE": "How many top documents to retrieve during search.",
  "SEARCH_TYPE": "The retrieval strategy (MMR, similarity, threshold, hybrid).",
  "SCORE_THRESHOLD": "Minimum similarity score for a document to be included.",
  "LAMBDA_MULT": "MMR balancing factor between relevance and diversity.",
  "DOMAIN_SIMILARITY_THRESHOLD": "Threshold for semantic similarity to your domain centroid.",
  "SPARSE_RELEVANCE_THRESHOLD": "Threshold for sparse keyword relevance in hybrid search.",
  "FUSED_RELEVANCE_THRESHOLD": "Combined score threshold when fusing semantic and sparse scores.",
  "SEMANTIC_WEIGHT": "Weight for semantic similarity when combining scores.",
  "SPARSE_WEIGHT": "Weight for sparse keyword matching when combining scores.",
  "PERFORM_DOMAIN_CHECK": "Enable domain relevance filtering before generating a response.",
  "PERFORM_TECHNICAL_VALIDATION": "Enable additional checks for technical consistency in the answer.",
  "SENTENCE_TRANSFORMER_MODEL": "Alias for your embedding model used during keyword extraction. Should match your main embedding setting.",
  "KEYWORDS_PER_DOCUMENT": "Max candidate keywords pulled from each doc before global filtering. Aim for 10–20.",
  "FINAL_KEYWORDS_COUNT": "Target total unique keywords across the entire corpus. 100–5000+, depending on size.",
  "MINIMUM_DOCUMENT_FREQUENCY": "Drop any keyword that appears in fewer than this many docs (absolute number or fraction, e.g. 3 or 0.01).",
  "DIVERSITY": "In final keyword MMR, balance relevance vs. diversity (0 = pure relevance, 1 = pure diversity). 0.5–0.8 is typical."
};
  
